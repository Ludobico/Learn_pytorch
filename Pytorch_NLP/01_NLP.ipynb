{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리란\n",
    "\n",
    "자연어 처리란 우리가 일상생활에서 사용하는 언어 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 과정입니다. 자연어 처리는 딥러닝에 대한 이해도 필요하지만, 그에 앞서 인간 언어에 대한 이해도 필요하기 때문에 접근하기 어려운 분야입니다. 또한, 언어 종류가 다르고 그 형태가 다양하기 때문에 처리가 매우 어렵습니다. 예를 들어 영어는 명확한 띄어쓰기가 있지만, 중국어는 띄어쓰기가 없기 때문에 단어 단위의 `임베딩`이 어렵습니다. 또한, 자연어 처리를 위해 사용되는 용어들도 낯섭니다.\n",
    "\n",
    "다음 그림은 자연어 처리가 가능한 영역과 발전이 필요한 분야입니다. 예를 들어 스팸 처리 및 맞춤법 검사는 완성도가 높은 반면, 질의응답 및 대화는 아직 발전이 더 필요한 분야입니다.\n",
    "\n",
    "![](../Static/506.jpg)\n",
    "\n",
    "자연어 처리에서 사용되는 `용어`부터 알아보겠습니다.\n",
    "\n",
    "## 자연어 처리 관련 용어\n",
    "\n",
    "* 말뭉치(corpus(코퍼스)) : 자연어 처리에서 모델을 학습시키기 위한 데이터이며, 자연어 연구를 위해 특정한 목적에서 표본을 추출한 집합입니다.\n",
    "\n",
    "![](../Static/507_1.jpg)\n",
    "\n",
    "* 토큰(token) : 자연어 처리를 위한 문서는 작은 단위로 나누어야 하는데, 이때 `문서를 나누는 단위`가 토큰입니다. 문자열을 토큰으로 나누는 작업을 토큰 생성(tokenizing)이라고 하며, 문자열을 토큰으로 분리하는 함수를 토큰 생성 함수 라고 합니다.\n",
    "\n",
    "* 토큰화(tokenization) : 텍스트를 문장이나 단어로 분리하는 것을 의미합니다. 토큰화 단계를 마치면 텍스트가 단어 단위로 분리됩니다.\n",
    "\n",
    "* 불용어(stop words) : 문장 내에서 많이 등장하는 단어입니다. 분석과 관계없으며, 자주 등장하는 빈도 때문에 성능에 영향을 미치므로 사전에 제거해 주셔야합니다. 불용어 예로 'a' ,'the' , 'she' , 'he' 등이 있습니다.\n",
    "\n",
    "* 어간 추출(stemming) : 단어를 기본 형태로 만드는 작업입니다. 예를 들어 'consign', 'consigned' , 'consigning', 'consignment' 가 있을때 기본 단어인 'consign'으로 통일하는 것이 어간 추출입니다.\n",
    "\n",
    "![](../Static/507_2.jpg)\n",
    "\n",
    "* 품사 태깅(part-of-speech tagging) : 주어진 문장에서 품사를 식별하기 위해 붙여 주는 태그를 의미합니다.\n",
    "\n",
    "![](../Static/508.jpg)\n",
    "\n",
    "    품사 태깅을 위한 정보는 다음과 같습니다.\n",
    "      * Det : 한정사, 관사\n",
    "      * Noun : 명사\n",
    "      * Verb : 동사\n",
    "      * Prep : 전치사\n",
    "\n",
    "품사 태깅은 `NLTK`를 이용할 수 있습니다.\n",
    "\n",
    "NLTK는 아나콘다가 설치되어 있다면 추가적으로 설치할 필요가 없지만, 책에서는 가상 환경에서 실습하므로 다음 명령으로 설치합니다.\n",
    "\n",
    "> pip install nltk\n",
    "\n",
    "품사 태깅을 위해 주어진 문장에 대해 토큰화를 먼저 진행합니다. 다음 코드를 실행하면 NLTK Downloader 창이 뜹니다. **Download** 를 눌러 내려받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Is', 'it', 'possible', 'distinguishing', 'cats', 'and', 'dogs', '?']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()\n",
    "text = nltk.word_tokenize('Is it possible distinguishing cats and dogs ?')\n",
    "text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "태깅에 필요한 자원을 내려받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\aqs45\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger') # 태깅에 필요한 자원 내려받기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내려받은 자원을 이용하여 품사를 태깅합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('possible', 'JJ'),\n",
       " ('distinguishing', 'VBG'),\n",
       " ('cats', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('dogs', 'NNS'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서 사용되는 품사 의미는 다음과 같습니다.\n",
    "\n",
    "* VBZ : 동사, 동명사 또는 현재 분사\n",
    "* PRP : 인칭 대명사(PP)\n",
    "* JJ : 형용사\n",
    "* VBG : 동사, 동명사 또는 현재 분사\n",
    "* NNS : 복수형 명사\n",
    "* CC : 등위 접속사\n",
    "\n",
    "## 자연어 처리 과정\n",
    "\n",
    "자연어는 인간 언어입니다. 인간 언어는 컴퓨터가 이해할 수 없기 때문에 컴퓨터가 이해할 수 있는 언어로 바꾸고 원하는 결과를 얻기까지 크게 네 단계를 걸칩니다.\n",
    "\n",
    "첫 번째로 인간 언어인 자연어가 입력 텍스트로 들어오게 됩니다. 이때 인간 언어가 다양하듯 처리 방식이 조금씩 다르며, 현재는 영어에 대한 처리 방법들이 잘 알려져있습니다.\n",
    "\n",
    "두 번째로는 입력된 텍스트에 대한 전처리 과정이 필요합니다.\n",
    "\n",
    "세 번째로 전처리가 끝난 단어들을 임베딩합니다. 즉, 단어를 벡터로 변환하는 방법으로 `자연어 처리를 위한 임베딩` 에서 자세히 다룹니다.\n",
    "\n",
    "마지막으로 컴퓨터가 이해할 수 있는 데이터가 완성되었기 때문에 모델/모형을 이용하여 데이터에 대한 분류 및 예측을 수행합니다. 이때 데이터 유형에 따라 분류와 예측에 대한 결과가 달라집니다.\n",
    "\n",
    "![](../Static/511.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ba10514d92c171690b73ca0a9c097f69f78d6fa5f41507cb6917f0a9599880a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet-5\n",
    "LeNet-5를 사용하는 예제를 구현해 봅시다. 우리가 구현할 신경망은 다음 그림과 같습니다.\n",
    "\n",
    "![](../Static/247.jpg)\n",
    "\n",
    "32x32 크기의 이미지에 합성곱층과 최대 풀링층이 쌍으로 두 번 적용된 후 완전연결층을 거쳐 이미지가 분류되는 신경망입니다.\n",
    "\n",
    "신경망에 대한 자세한 설명은 다음 표와 같습니다.\n",
    "\n",
    "|계층 유형|특성 맵|크기|커널 크기|스트라이드|활성화 함수|\n",
    "|---|---|---|---|---|---|\n",
    "|이미지|1|32x32|-|-|-|\n",
    "|합성곱층|6|28x28|5x5|1|렐루|\n",
    "|최대 풀링층|6|14x14|2x2|2|-|\n",
    "|합성곱층|16|10x10|5x5|1|렐루|\n",
    "|최대 풀링층|16|5x5|2x2|2|-|\n",
    "|완전연결층|-|120|-|-|렐루|\n",
    "|완전연결층|-|84|-|-|렐루|\n",
    "|완전연결층|-|2|-|-|소프트맥스|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제를 진행하기 위해 프롬프트에서 `tqdm` 라이브러리를 설치해 주세요. 라이브러리는 다음과 같이 두 가지 방법으로 설치할 수 있습니다.\n",
    "\n",
    "> pip install tqdm\n",
    "\n",
    "> conda install -c conda-forge tqdm\n",
    "\n",
    "tqdm은 아랍어로 progress(진행 상태)라고도 합니다. 즉, 진행 상태를 바(bar) 형태로 가시화하여 보여 줍니다. 주로 `모델 훈련에 대한 진행 상태`를 확인하고자 할 때 사용합니다.\n",
    "\n",
    "설치가 완료되었다면 필요한 라이브러리를 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import dataset, DataLoader\n",
    "import torchvision.transforms as transforms # 이미지 변환(전처리) 기능을 제공하는 라이브러리\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm # 진행 상황을 가시적으로 표현\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 모델 학습에 필요한 데이터셋의 전처리(텐서 변환)이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = './seg_train/seg_train'\n",
    "test_data_dir = './seg_test/seg_test'\n",
    "batch_size = 32\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(64),\n",
    "     transforms.CenterCrop(64),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_data_dir,\n",
    "                                     transform=transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               drop_last=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(test_data_dir,\n",
    "                                    transform=transform)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=False,\n",
    "                                              drop_last=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `토치비전(torchvision)` 라이브러리를 이용하면 이미지에 대한 전처리를 손쉽게 할 수 있습니다. `torchvision.transforms` 에서 사용하는 파라미터는 다음과 같습니다.\n",
    "    * `transforms.Compose` 이미지를 변형할 수 있는 방식들의 묶음입니다.\n",
    "    * `transforms.RandomResizedCrop` 입력 이미지를 주어진크기(resize:224x224)로 조정합니다. 또한, `scale`은 원래 이미지를 임의의 크기(0.5~1.0) 만큼 면적을 무작위로 자르겠다는 의미입니다.\n",
    "    * `transforms.RandomHorizontalFlip` 주어진 확률로 이미지를 수평 반전시킵니다. 이때 확률 값을 지정하지 않았으므로 기본값인 0.5의 확률로 이미지들이 수평 반전됩ㄴ다. 즉, 훈련 이미지 중 반은 위아래 뒤집한 상태로 두고, 반은 그대로 사용합니다.\n",
    "    * `transforms.ToTensor()` ImageFolder 메서드를 비롯해서 torchvision 메서드는 이미지를 읽을 때 파이썬 이미지 라이브러리인 PIL을 사용합니다. PIL을 사용해서 이미지를 읽으면 생성되는 이미지는 범위가 [0,255]이며, 배열의 차원이 `(높이 H x 너비 W x 채널 수 C)`로 표현됩니다. 이후 효율적인 연산을 위해 `torch.FloatTensor` 배열로 바꾸어야 하는데, 이때 픽셀 값의 범위는 [0.0, 1.0] 사이가 되고 차원의 순서도 `(채널 수 C x 높이 H x 너비 W)`로 바뀝니다. 그리고 이러한 작업을 수행해 주는 메서드가 `ToTensor()`입니다.\n",
    "    * `transforms.Normalize` 전이 학습에서 사용되는 사전 훈련된 모델들은 대게 `ImageNet 데이터셋`에서 훈련되었습니다. 따라서 사전 훈련된 모델을 사용하기 위해서는 ImageNet 데이터의 각 채널별 평균과 표준편차에 맞는 `정규화(Normalize)`를 해 주어야 합니다. 참고로 openCV를 사용해서 이미지를 읽어 온다면 RGB이미지가 아닌 BGR 이미지이므로 채널 순서에 주의해야 합니다.\n",
    "\n",
    "* `__call__` 함수는 클래스를 호출할 수 있도록 하는 메서드입니다. __init__은 인스턴스 초기화를 위해 사용한다면 __call__은 인스턴스가 호출되었을 때 실행됩니다. 즉, 클래스에 __call__ 함수가 있을 경우 클래스 객체 자체를 호출하면 __call__함수의 리턴(return) 값이 반환됩니다.\n",
    "\n",
    "이미지가 위치한 디렉터리에서 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  32\n",
      "Image Size:  torch.Size([32, 3, 64, 64])\n",
      "tensor([5, 3, 4, 5, 3, 4, 2, 4, 0, 2, 4, 0, 1, 3, 1, 5, 1, 4, 4, 2, 3, 0, 4, 2,\n",
      "        5, 4, 2, 5, 4, 5, 0, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "print('Number of samples: ', len(images))\n",
    "\n",
    "image = images[2][0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "\n",
    "print(\"Image Size: \", images.shape)\n",
    "\n",
    "print(labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터셋을 학습시킬 모델의 네트워크를 설계하기 위한 클래스를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1)\n",
    "        # 파라미터 수 : (kernel_size *2) * in_c * out_c + out_c\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1)\n",
    "        # 파라미터 수 : (kernel_size *2) * in_c * out_c + out_c\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1)\n",
    "        # 파라미터 수 : (kernel_size *2) * in_c * out_c + out_c\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=1)\n",
    "        # 파라미터 수 : (kernel_size *2) * in_c * out_c + out_c\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=512 * 4 * 4, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=6)\n",
    "        # 파라미터 수 : in_f * out_f + out_f\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class CNN_MK2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_MK2, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3,64,3,padding=1,stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(512 * 4 * 4, 128)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=8192, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n",
      "CNN_MK2(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=8192, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device=device)\n",
    "print(model)\n",
    "\n",
    "modelMK2 = CNN_MK2().to(device)\n",
    "print(modelMK2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 결과가 한눈에 들어오지 않는다면 `torchsummary` 라이브러리를 사용해 볼 수 있습니다. `torchsummary`는 케라스와 같은 형태로 모델을 출력해 볼 수 있는 라이브러리입니다.\n",
    "\n",
    "먼저 라이브러리를 설치합니다.\n",
    "> pip install torchsummary\n",
    "\n",
    "설치가 완료되었다면 torchsummary 라이브러리를 사용하여 모델의 네트워크 구조를 다시 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           1,792\n",
      "         MaxPool2d-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3          [-1, 128, 32, 32]          73,856\n",
      "         MaxPool2d-4          [-1, 128, 16, 16]               0\n",
      "            Conv2d-5          [-1, 256, 16, 16]         295,168\n",
      "         MaxPool2d-6            [-1, 256, 8, 8]               0\n",
      "            Conv2d-7            [-1, 512, 8, 8]       1,180,160\n",
      "         MaxPool2d-8            [-1, 512, 4, 4]               0\n",
      "            Linear-9                  [-1, 128]       1,048,704\n",
      "          Dropout-10                  [-1, 128]               0\n",
      "           Linear-11                    [-1, 6]             774\n",
      "================================================================\n",
      "Total params: 2,600,454\n",
      "Trainable params: 2,600,454\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 4.69\n",
      "Params size (MB): 9.92\n",
      "Estimated Total Size (MB): 14.66\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           1,792\n",
      "              ReLU-2           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-3           [-1, 64, 32, 32]               0\n",
      "            Conv2d-4          [-1, 128, 32, 32]          73,856\n",
      "              ReLU-5          [-1, 128, 32, 32]               0\n",
      "         MaxPool2d-6          [-1, 128, 16, 16]               0\n",
      "            Conv2d-7          [-1, 256, 16, 16]         295,168\n",
      "              ReLU-8          [-1, 256, 16, 16]               0\n",
      "         MaxPool2d-9            [-1, 256, 8, 8]               0\n",
      "           Conv2d-10            [-1, 512, 8, 8]       1,180,160\n",
      "             ReLU-11            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-12            [-1, 512, 4, 4]               0\n",
      "           Linear-13                  [-1, 128]       1,048,704\n",
      "          Dropout-14                  [-1, 128]               0\n",
      "           Linear-15                    [-1, 6]             774\n",
      "================================================================\n",
      "Total params: 2,600,454\n",
      "Trainable params: 2,600,454\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 8.44\n",
      "Params size (MB): 9.92\n",
      "Estimated Total Size (MB): 18.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(modelMK2, input_size=(3, 64, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, criterion, optimizer, device=device, dataloader=train_dataloader):\n",
    "\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.data\n",
    "        else:\n",
    "            print(f\" Epoch {e}, Training loss: {running_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 0, Training loss: 1.2665506601333618\n",
      " Epoch 1, Training loss: 0.9253822565078735\n",
      " Epoch 2, Training loss: 0.7735886573791504\n",
      " Epoch 3, Training loss: 0.6715831160545349\n",
      " Epoch 4, Training loss: 0.5863961577415466\n",
      " Epoch 5, Training loss: 0.5069655179977417\n",
      " Epoch 6, Training loss: 0.4486648142337799\n",
      " Epoch 7, Training loss: 0.39137211441993713\n",
      " Epoch 8, Training loss: 0.3335861265659332\n",
      " Epoch 9, Training loss: 0.28163942694664\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "model = CNN().to(device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "fit(epochs, model, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, batch_size=32, device=device, dataloader=test_dataloader):\n",
    "    classes = ('buildings', 'forest', 'glacier', 'mountain',\n",
    "               'sea', 'street')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        n_class_correct = [0 for i in range(6)]\n",
    "        n_class_samples = [0 for i in range(6)]\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "                if (label == pred):\n",
    "                    n_class_correct[label] += 1\n",
    "                n_class_samples[label] += 1\n",
    "\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "        for i in range(6):\n",
    "            acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "            print(f'Accuracy of {classes[i]}: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 79.80510752688173 %\n",
      "Accuracy of buildings: 79.40503432494279 %\n",
      "Accuracy of forest: 81.64556962025317 %\n",
      "Accuracy of glacier: 72.15189873417721 %\n",
      "Accuracy of mountain: 71.80952380952381 %\n",
      "Accuracy of sea: 89.6078431372549 %\n",
      "Accuracy of street: 85.53459119496856 %\n"
     ]
    }
   ],
   "source": [
    "predict(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Learn_pytorch\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define input tensor\n",
    "input_tensor = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# define convolutional layer\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=32,\n",
    "                       kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# pass input tensor through the convolutional layer\n",
    "output_tensor = conv_layer(input_tensor)\n",
    "\n",
    "# check the size of the output tensor\n",
    "print(output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ba10514d92c171690b73ca0a9c097f69f78d6fa5f41507cb6917f0a9599880a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

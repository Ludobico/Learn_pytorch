{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq\n",
    "\n",
    "seq2seq(sequence to sequence)는 입력 시퀀스(input sequence)에 대한 출력 시퀀스(output sequence)를 만들기 위한 모델입니다. seq2seq는 품사 판별과 같은 시퀀스 레이블링(sequence labeling)과는 차이가 있습니다. 시퀀스 레이블링이란 입력 단어가 $x_1$, $x_2$, ... , $x_n$ 이라면 출력은 $y_1$, $y_2$, ... , $y_n$ 이 되는 형태입니다. 즉, 입력과 출력에 대한 문자열(sequence)이 같습니다. 하지만 seq2seq은 품사 판별보다는 `번역`에 초점을 둔 모델입니다. 번역은 입력 시퀀스의 $x_{1:n}$과 의미가 동일한 출력 시퀀스 $y_{1:m}$을 만드는 것이며, $x_i$, $y_i$ 간의 관계는 중요하지 않습니다. 그리고 각 시퀀스 길이도 서로 다를 수 있습니다.\n",
    "\n",
    "![](../Static/568.jpg)\n",
    "\n",
    "그럼 지금부터 seq2seq를 파이토치로 구현해 보겠습니다. 영어를 프랑스어로 번역하는 예제입니다. 이 예제는 파이토치 튜토리얼에 게시된 코드를 수정한 것입니다. 튜토리얼 코드와 비교하면서 학습해도 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `__future__` 는 구 버전에서 상위 버전의 기능을 이용해야 할 때 사용합니다. 모듈을 import 하여 사용하는 것처럼 __future__를 import 하여 상위 버전의 기능을 사용합니다. 물론 최신 버전의 파이토치를 사용하는 경우에는 필요하지 않습니다. 예제는 사용 방법을 익히기 위해 추가해 두었습니다.\n",
    "\n",
    "* re모듈은 `정규표현식`을 사용하고자 할 때 씁니다.\n",
    "\n",
    "데이터셋은 타토에바 프로젝트 중에서 영어-프랑스어 파일을 사용합니다. 다음 URL에서 다양한 언어에 대한 것들을 제공하기 있기 때문에 예제에서 사용하는 영어-프랑스어 외에도 다른 언어를 내려받아 사용할 수 있습니다. 물론 영어-한국어도 제공합니다.\n",
    "\n",
    "http://www.manythings.org/anki/\n",
    "\n",
    "파이토치에서는 문장 그대로 사용할 수 없습니다. 문장을 단어로 분할하고 벡터(vector)로 변환해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "\n",
    "class Lang: # 딕셔너리를 만들기 위한 클래스\n",
    "    def __init__(self): # 단어의 인덱스를 저장하기 위한 컨테이너 초기화\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"} # sos(문장의 시작), eos(문장의 끝)\n",
    "        self.n_words = 2 #sos와 eos에 대한 카운트\n",
    "\n",
    "    def addSentence(self, sentence): # 문장을 단어 단위로 분리한 후 컨테이너(word)에 추가\n",
    "       for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word): # 컨테이너에 단어가 없다면 추가되고, 있다면 카운트를 업데이트\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋으로 사용된 데이터는 앞에서 살펴보았듯이 탭(tab)으로 구성된 text 파일입니다. 따라서 데이터는 판다스로 불러온 후 정규화해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(df, lang):\n",
    "    sentence = df[lang].str.lower() # 소문자로 변환\n",
    "    sentence = sentence.str.replace('[^A-Za-z\\s]+', ' ')\n",
    "    sentence = sentence.str.normalize('NFD') # 유니코드 정규화 방식\n",
    "    sentence = sentence.str.encode('ascii', errors='ignore').str.decode('utf-8') # 유니코드를 ascii로 전환\n",
    "    return sentence\n",
    "\n",
    "def read_sentence(df, lang1, lang2):\n",
    "    sentence1 = normalizeString(df, lang1) # 데이터의 첫 번째 열 (영어)\n",
    "    sentence2 = normalizeString(df, lang2) # 데이터의 두 번째 열 (선택한 언어)\n",
    "    return sentence1, sentence2\n",
    "\n",
    "def read_file(loc, lang1, lang2):\n",
    "    df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2,'cc'])\n",
    "    df.drop(axis=1, columns='cc', inplace=True)\n",
    "    return df\n",
    "\n",
    "def process_data(lang1, lang2): # 데이터셋 불러오기\n",
    "    df = read_file('../fra-eng/fra.txt', lang1, lang2)\n",
    "    sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
    "\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang()\n",
    "    pairs = []\n",
    "    for i in range(len(df)):\n",
    "        if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n",
    "            full = [sentence1[i], sentence2[i]]\n",
    "            input_lang.addSentence(sentence1[i])\n",
    "            output_lang.addSentence(sentence2[i])\n",
    "            pairs.append(full)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2,'cc'])\n",
    "    * loc : 예제에서 사용할 데이터셋\n",
    "    * delimiter : CSV파일의 데이터가 어떤 형태(\\t, ' ','+' 등)로 나뉘었는지 의미합니다. 데이터를 \" \" 묶음으로 처리할 때 사용합니다. 예를 들어 \"Sure, I'm OK\" 처럼 문자열에 콤마가 포함되어 있을 경우 \"Sure\"와 \"I'm OK\"로 나뉘는데, 이를 방지할 수 있습니다. 즉, 하나의 문장이 분할되지 않고 그대로 사용하고 싶을 때 유용합니다.\n",
    "    * header : 일반적으로 데이터셋의 첫 번째 행을 header(열 이름)로 지정해서 사용하게 되는데, 불러올 데이터에 header가 없을 경우 `header=None` 옵션을 사용합니다.\n",
    "    * names : 열 이름을 리스트 형태로 입력합니다. 데이터셋의 총 세 개의 열이 있기 때문에 lang1, lang2 그리고 저작권열인 cc를 입력하고 cc를 드랍합니다.\n",
    "\n",
    "이제 데이터 쌍(paris)을 텐서로 변환해야 합니다. 계속 이야기하지만 파이토치의 네트워크는 텐서 유형의 데이터만 인식하기 때문에 매우 중요한 작업입니다. 이 작업은 중요한 또 다른 이유는 지금 진행하고 있는 데이터셋이 문장이기 때문입니다. 따라서 문장의 모든 끝에 입력이 완료되었음을 네트워크에 알려 주어야 하는데, 그것이 토큰입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence): # 문장을 단어로 분리하고 인덱스를 반환\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence): # 딕셔너리에서 단어에 대한 인덱스를 가져오고 문장끝에 토큰을 추가\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(input_lang, output_lang, pair): # 입력과 출력 문장을 텐서로 변환하여 반환\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파이토치에서 seq2seq 모델을 사용하기 위해서는 먼저 인코더와 디코더를 정의해야 합니다.\n",
    "입력(영어) 문장이 인코더로 주입되면 디코더(프랑스어)로 번역되어 출력됩니다. 인코더와 디코더를 이용하면 문장의 번역뿐만 아니라 다음 입력을 예측하는 것도 가능합니다. 이때 각 입력 문장의 끝에는 문장의 끝을 알리는 토큰이 할당됩니다.\n",
    "\n",
    "![](../Static/572.jpg)\n",
    "\n",
    "`인코더`는 입력 문장을 단어별로 순서대로 인코딩을 하게 되며, 문장의 끝을 표시하는 토큰이 붙습니다. 또한, 인코더는 `임베딩 계층`과 `GRU 계층`으로 구성됩니다.\n",
    "\n",
    "![](../Static/573.jpg)\n",
    "\n",
    "임베딩 계층은 입력에 대한 임베딩 결과가 저장되어 있는 딕셔너리를 조회하는 테이블과도 같습니다. 이후 GRU 계층과 연결되는데, GRU 계층은 연속하여 들어오는 입력을 계산합니다. 또한, 이전 계층의 은닉 상태를 계산한 후 망각 게이트와 업데이트 게이트를 갱신합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_dim = input_dim # 인코더에서 사용할 입력층\n",
    "        self.embbed_dim = embbed_dim # 인코더에서 사용할 임베딩 계층\n",
    "        self.hidden_dim = hidden_dim # 인코더에서 사용할 은닉층(이전 은닉층)\n",
    "        self.num_layers = num_layers # 인코더에서 사용할 GRU의 계층 개수\n",
    "        self.embedding = nn.Embedding(input_dim, self.embbed_dim) # 임베딩 계층 초기화\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers) # 임베딩 차원, 은닉층 차원, GRU의 계층 개수를 이용하여 GRU 계층을 초기화\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src).view(1, 1, -1) # 임베딩 처리\n",
    "        outputs, hidden = self.gru(embedded) # 임베딩 결과를 GRU 모델에 적용\n",
    "        return outputs, hidden\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`디코더`는 인코더 출력을 디코딩하여 다음 출력을 예측합니다. 디코더는 임베딩 계층, GRU 계층, 선형(linear) 계층으로 구성됩니다.\n",
    "\n",
    "![](../Static/574.jpg)\n",
    "\n",
    "임베딩 계층에서는 출력을 위해 딕셔너리를 조회할 테이블을 만들며, GRU 계층에서는 다음단어를 예측하기 위한 확률을 계산합니다. 그 후 선형 계층에서는 계산된 확률 값 중 최적의 값(최종 출력 단어)을 선택하기 위해 소프트맥스 활성화 함수를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, self.embbed_dim) # 임베딩 계층 초기화\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers) # GRU 계층 초기화\n",
    "        self.out = nn.Linear(self.hidden_dim, output_dim) # 선형 계층 초기화\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.view(1, -1) # 입력을 (1, 배치 크기)로 변경\n",
    "        embedded = F.relu(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        prediction = self.softmax(self.out(output[0]))\n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* self.softmax = nn.LogSoftmax(dim=1)\n",
    "    * 소프트맥스는 일정한 시퀀스의 숫자들을 0과 1 사이의 양의 수로 변환해서 클래스의 확률을 구할 때 사용합니다.\n",
    "    * 로그 소프트맥스(LogSoftmax)는 소프트맥스와 로그 함수의 결합입니다.\n",
    "    * 소프트맥스 활성화 함수에서 발생할 수 있는 `기울기 소멸 문제를 방지`하기 위해 만들어진 활성화 함수입니다.\n",
    "\n",
    "앞에서 정의한 인코더와 디코더를 이용하여 seq2seq 모델을 정의합니다. 인코더와 디코더를 이용한 seq2seq 네트워크는 다음 그림과 같습니다.\n",
    "\n",
    "![](../Static/575.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder #인코더 초기화\n",
    "        self.decoder = decoder # 디코더 초기화\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_lang, output_lang, teacher_forcing_ratio=0.5):\n",
    "        input_length = input_lang.size(0) # 입력 문자 길이(문장의 단어 수)\n",
    "        batch_size = output_lang.shape[1]\n",
    "        target_length = output_lang.shape[0]\n",
    "        vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device) # 예측된 출력을 저장하기 위한 변수 초기화\n",
    "\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = self.encoder(input_lang[i]) # 문장의 모든 단어를 인코딩\n",
    "        decoder_hidden = encoder_hidden.to(device) # 인코더의 은닉층을 디코더의 은닉층으로 사용\n",
    "        decoder_input = torch.tensor([SOS_token], device=device) # 첫 번째 예측 단어 앞에 토큰(sos) 추가\n",
    "\n",
    "        for t in range(target_length): # 현재 단어에서 출력 단어를 예측\n",
    "            decoder_output, decoder_hidden = self.decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            outputs[t] = decoder_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            input = (output_lang[t] if teacher_force else topi) # teacher_force를 활성화 하면 목표 단어를 다음 입력으로 사용\n",
    "            if (teacher_force == False and input.item() == EOS_token): # teacher_force를 활성화하지 않으면 자체 예측 값을 다음 입력으로 사용\n",
    "                break\n",
    "        return outputs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  teacher_force = random.random() < teacher_forcing_ratio\n",
    "    * 티처포스(teacher_force)는 seq2seq(인코더-디코더) 모델에서 많이 사용되는 기법입니다. 티처포스는 다음 그림과 같이 번역(예측)하려는 목표 단어(ground truth)를 디코더의 다음 입력으로 넣어 주는 기법입니다.\n",
    "\n",
    "    ![](../Static/577.jpg)\n",
    "\n",
    "티처포스를 사용하면 학습 초기에 `안정적인 훈련`이 가능하며, 기울기를 계산할 때 빠른 수렴이 가능한 장점이 있지만 `네트워크가 불안정해질 수 있는 단점`이 있습니다.\n",
    "\n",
    "모델 훈련을 위한 함수를 정의합니다. 여기에서는 모델의 오차를 계산하는 부분만 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def Model(model, input_tensor, target_tensor, model_optimizer,  criterion):\n",
    "    model_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    loss = 0\n",
    "    epoch_loss = 0\n",
    "    output = model(input_tensor, target_tensor)\n",
    "    num_iter = output.size(0)\n",
    "\n",
    "    for ot in range(num_iter):\n",
    "        loss += criterion(output[ot], target_tensor[ot]) # 모델의 예측 결과와 정답(예상 결과)을 이용하여 오차를 계산\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    epoch_loss = loss.item() / num_iter\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, input_lang, output_lang, pairs, num_iteration=20000):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01) # 옵티마이저로 SGD를 사용\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss_iterations = 0\n",
    "\n",
    "    training_pairs = [tensorsFromPair(\n",
    "        input_lang, output_lang, random.choice(pairs)) for i in range(num_iteration)]\n",
    "\n",
    "    for iter in range(1, num_iteration+1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = Model(model, input_tensor, target_tensor, optimizer, criterion)\n",
    "        total_loss_iterations += loss\n",
    "\n",
    "        if iter % 5000 == 0:\n",
    "            average_loss = total_loss_iterations / 5000 # 5000번쨰마다 오차 값에 대해 출력\n",
    "            total_loss_iterations = 0\n",
    "            print('%d %.4f' % (iter, average_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), './mytraining.pt')\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* criterion = nn.NLLLoss()\n",
    "    * `NLLLoss` 역시 크로스엔트로피 손실 함수(CrossEntropyLoss)와 마찬가지로 `분류 문제`에 사용합니다. 이 둘간의 차이는 다음과 같습니다. 크로스엔트로피 손실 함수에는 `LogSoftmax + NLLLoss`가 포함되어 있습니다. 따라서 `크로스엔트로피 손실 함수를 사용할 경우에는 소프트맥스를 명시하지 않아도 되지만` NLLLoss를 사용할 때는 사용자가 소프트맥스를 사용할 것임을 명시해야 합닏. 이러한 이유로 모델 네트워크 부분에서도 소프트맥스 활성화 함수를 지정해 주었습니다.\n",
    "\n",
    "이제 모델을 평가하기 위한 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentences[0]) # 입력 문자열을 텐서로 변환\n",
    "        output_tensor = tensorFromSentence(output_lang, sentences[1]) # 출력 문자열을 텐서로 변환\n",
    "        decoded_words = []\n",
    "        output = model(input_tensor, output_tensor)\n",
    "\n",
    "        for ot in range(output.size(0)):\n",
    "            topv, topi = output[ot].topk(1) # 각 출력에서 가장 높은 값을 찾아 인덱스를 반환\n",
    "\n",
    "            if topi[0].item() == EOS_token:\n",
    "                decoded_words.append('<EOS>') # EOS 토큰을 만나면 평가를 멈춤\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi[0].item()]) # 예측 결과를 출력 문자열에 추가\n",
    "\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateRandomly(model, input_lang, output_lang, pairs, n=10): # 훈련 데이터셋으로부터 임의의 문장을 가져와서 모델 평가\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs) # 임의의 문장을 가져옴\n",
    "        print('input {}'.format(pair[0]))\n",
    "        print('output {}'.format(pair[1]))\n",
    "        output_words = evaluate(model, input_lang, output_lang, pair) # 모델 평가 결과는 output_words에 저장\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('predicted {}'.format(output_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aqs45\\AppData\\Local\\Temp\\ipykernel_3896\\1179822021.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  sentence = sentence.str.replace('[^A-Za-z\\s]+', ' ')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sentence ['i don t think i can fix this ', 'je ne pense pas pouvoir r parer  a ']\n",
      "Input : 14869 Output : 21050\n",
      "Encoder(\n",
      "  (embedding): Embedding(14869, 256)\n",
      "  (gru): GRU(256, 512)\n",
      ")\n",
      "Decoder(\n",
      "  (embedding): Embedding(21050, 256)\n",
      "  (gru): GRU(256, 512)\n",
      "  (out): Linear(in_features=512, out_features=21050, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "5000 4.9042\n",
      "10000 4.8358\n",
      "15000 4.8212\n",
      "20000 4.8255\n",
      "25000 4.8455\n",
      "30000 4.8263\n",
      "35000 4.7858\n",
      "40000 4.7962\n",
      "45000 4.8054\n",
      "50000 4.8085\n",
      "55000 4.7657\n",
      "60000 4.8037\n",
      "65000 4.7917\n",
      "70000 4.8253\n",
      "75000 4.7844\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./seq2seqResult does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(encoder)\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(decoder)\n\u001b[1;32m---> 24\u001b[0m model \u001b[39m=\u001b[39m trainModel(model, input_lang, output_lang, pairs, num_iteration)\n",
      "Cell \u001b[1;32mIn[27], line 22\u001b[0m, in \u001b[0;36mtrainModel\u001b[1;34m(model, input_lang, output_lang, pairs, num_iteration)\u001b[0m\n\u001b[0;32m     19\u001b[0m         total_loss_iterations \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     20\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%.4f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39miter\u001b[39m, average_loss))\n\u001b[1;32m---> 22\u001b[0m torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), \u001b[39m'\u001b[39;49m\u001b[39m./seq2seqResult/mytraining.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     23\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Learn_pytorch\\venv\\lib\\site-packages\\torch\\serialization.py:422\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    419\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m    421\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 422\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    423\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    424\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Learn_pytorch\\venv\\lib\\site-packages\\torch\\serialization.py:309\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[1;32mc:\\Users\\aqs45\\OneDrive\\바탕 화면\\repo\\Learn_pytorch\\venv\\lib\\site-packages\\torch\\serialization.py:287\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m     \u001b[39msuper\u001b[39m(_open_zipfile_writer_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory ./seq2seqResult does not exist."
     ]
    }
   ],
   "source": [
    "lang1 = 'eng' # 입력으로 사용할 영어\n",
    "lang2 = 'fra' # 출력으로 사용할 프랑스어\n",
    "input_lang, output_lang, pairs = process_data(lang1, lang2)\n",
    "\n",
    "randomize = random.choice(pairs)\n",
    "print('random sentence {}'.format(randomize))\n",
    "\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "print('Input : {} Output : {}'.format(input_size, output_size)) # 입력과 출력에 대한 단어 수 출력\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "num_iteration = 75000 \n",
    "\n",
    "encoder = Encoder(input_size, hidden_size, embed_size, num_layers) # 인코더에 훈련 데이터셋을 입력하고 모든 출력과 은닉 상태를 저장\n",
    "# 디코더의 첫 번째 입력으로 <SOS> 토큰이 제공되고, 인코더의 마지막 은닉 상태가 디코더의 첫 번째 은닉 상태로 제공됩니다.\n",
    "decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)  # 인코더-디코더 모델(seq2seq)의 객체 생성\n",
    "\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "model = trainModel(model, input_lang, output_lang, pairs, num_iteration) # 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ba10514d92c171690b73ca0a9c097f69f78d6fa5f41507cb6917f0a9599880a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

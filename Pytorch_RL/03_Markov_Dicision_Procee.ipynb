{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 마르코프 결정 과정\n",
    "\n",
    "마르코프 결정 과정(markov dicision process, MDP)은 기존 마르코프 보상 과정에서 행동이 추가된 확률 모델입니다. MDP 목표는 정의된 문제에 대해 각 상태마다 전체적인 보상을 최대화하는 행동이 무엇인지를 결정하는 것입니다. 이때 각각의 상태마다 행동 분포(행동이 선택될 확률)를 표현하는 함수를 `정책`(policy, $\\pi$)이라고 하며, $\\pi$는 주어진 상태 $s$에 대한 행동 분포를 표현한 것으로 수식은 다음과 같습니다.\n",
    "\n",
    "$\\pi(a|s) = P(A_t = a | S_t = s)$\n",
    "\n",
    "MDP가 주어진 $\\pi$를 따를 때 $s$에서 $s`$로 이동할 확률은 다음 수식으로 계산됩니다.\n",
    "\n",
    "![](../Static/fn2-39.jpg)\n",
    "\n",
    "이때 $s$에서 얻을 수 있는 보상(R)은 다음과 같습니다.\n",
    "\n",
    "![](../Static/fn2-50.jpg)\n",
    "\n",
    "또한, MDP를 잏하기 위해서는 가치 함수도 이해해야 합니다. MDP에서 가치 함수는 에이전트가 놓인 상태 가치를 함수로 표현한 상태-가치 함수(state-value function)와 상태와 행동에 대한 가치를 함수로 표현한 행동-가치 함수(action-value function)가 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상태-가치 함수\n",
    "\n",
    "MDP에서 상태-가치 함수($v_{\\pi}(s)$)는 MRP의 가치 함수와 마찬가지로 상태 $s$에서 얻을 수 있는 `리턴의 기댓값`을 의미합니다. 하지만 MRP와 차이는 주어진 정책($\\pi$)에 따라 행동을 결정하고 다음 상태로 이동하기 때문에 MDP에서 상태-가치 함수는 다음 수식을 사용합니다.\n",
    "\n",
    "![](../Static/fn2-12.jpg)\n",
    "\n",
    "### 행동-가치 함수\n",
    "\n",
    "행동-가치 함수($q_{\\pi}(s,a)$)는 상태 $s$에서 $a$라는 행동을 취했을 때 얻을 수 있는 리턴의 기댓값을 의미합니다. 행동-가치 함수에 대한 수식은 다음과 같습니다.\n",
    "\n",
    "![](../Static/fn2-55.jpg)\n",
    "\n",
    "가치 함수(상태-가치 함수, 행동-가치 함수)를 계산하는 방법은 $O(n^3)$ 시간 복잡도가 필요하기 때문에 상태 수가 많으면 적용하기가 어려운데, 이 문제를 해결하는 방법은 다음 네 가지입니다. 각 방법론은 이전 단계의 단점을 보완하고 학습 효율을 높이는 방향으로 발전되었습니다.\n",
    "\n",
    "1. **다이나믹 프로그래밍**(dynamic programming) : 마르코프 결정 과정의 상태와 행동이 많지 않고 모든 상태와 전이 확률을 알고 있다면 다이나믹 프로그래밍 방식으로 각 상태의 가치와 최적의 행동을 찾을 수 있습니다. 하지만 대부분의 강화 학습 문제는 상태도 많고, 상태가 전이되는 경우의 수도 많으므로 다이나믹 프로그래밍을 적용하기 어렵습니다.\n",
    "\n",
    "2. **몬테카를로**(Monte Carlo method) : 마르코프 결정 과정에서 상태가 많거나 모든 상태를 알 수 없는 경우에는 다이나믹 프로그래밍을 적용할 수 없었습니다. 몬테카를로는 전체 상태 중 일부 구간만 방문하여 근사적으로 가치를 추정합니다. 초기 상태에서 시작하여 중간 상태들을 경유해서 최종(terminal)상태까지 간 후 최종 보상을 측정하고 방문했던 상태들의 가치를 업데이트합니다.\n",
    "\n",
    "3. **시간 차 학습**(temporal difference learning) : 몬테카를로는 최종 상태까지 도달한 후에야 방문한 상태들의 업데이트가 가능하다는 단점이 있습니다. 시간 차 학습 방식은 최종 상태에 도달하기 전에 방문한 상태의 가치를 즉시 업데이트합니다. 즉, 시간 차 학습은 다이나믹 프로그래밍과 몬테카를로의 중간적인 특성을 가지며 본격적인 강화 학습의 단계라고 할 수 있습니다.\n",
    "\n",
    "4. **함수적 접근 학습**(function approximation learning) : 마르코프 결정 과정의 상태가 아주 많거나, 상태가 연속적인 값을 갖는 경우는 상태-가치 함수나 행동-가치 함수를 테이블 형태로 학습하기 어렵습니다. 함수적 접근 학습 방법은 연속적인 상태를 학습하고자 상태와 관련된 특성벡터를 도입했습니다. 특성의 가중치를 업데이트하여 가치의 근사치를 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
